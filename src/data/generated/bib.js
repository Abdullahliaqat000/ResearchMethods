const generatedBibEntries = {
    "AquinoBritez2025EnergyConsumption": {
    "abstract": "The growing global demand for computational resources, particularly in Artificial Intelligence (AI) applications, raises increasing concerns about energy consumption and its environmental impact. This study introduces a newly developed energy consumption index that evaluates the energy efficiency of Deep Learning (DL) models, providing a standardized and adaptable approach for various models. Convolutional neural networks, including both classical and modern architectures, serve as the primary case study to demonstrate the applicability of the index. Furthermore, the inclusion of the Swin Transformer, a state-of-the-art and modern non-convolutional model, highlights the adaptability of the framework to diverse architectural paradigms. This study analyzes the energy consumption during both training and inference of representative DL architectures, including AlexNet, ResNet18, VGG16, EfficientNet-B3, ConvNeXt-T, and Swin Transformer, trained on the Imagenette dataset using TITAN XP and GTX 1080 GPUs. Energy measurements are obtained using sensor-based tools, including OpenZmeter (v2) with integrated electrical sensors. Additionally, software-based tools such as CarbonTracker (v1.2.5) and CodeCarbon (v2.4.1) retrieve energy consumption data from computational component sensors. The results reveal significant differences in energy efficiency across architectures and GPUs, providing insights into the trade-offs between model performance and energy use. By offering a flexible framework for comparing energy efficiency across DL models, this study advances sustainability in AI systems, supporting accurate and standardized energy evaluations applicable to various computational settings.",
    "author": "Sergio Aquino-Brítez, Pablo García-Sánchez, Andrés Ortiz, Diego Aquino-Brítez",
    "doi": "10.3390/s25030846",
    "journal": "Sensors",
    "keywords": "green computing, energy efficiency, machine learning, deep learning, convolutional neural network",
    "number": "3",
    "publisher": "MDPI",
    "series": "",
    "title": "Towards an Energy Consumption Index for Deep Learning Models: A Comparative Analysis of Architectures, GPUs, and Measurement Tools",
    "type": "article",
    "url": "https://doi.org/10.3390/s25030846",
    "volume": "25",
    "year": "2025"
  },
  "Brownlee2021AccuracyEnergyTradeoff": {
    "abstract": "Machine learning accounts for considerable global electricity demand and resulting environmental impact, as training a large deep-learning model produces 284000kgs of the greenhouse gas carbon dioxide. In recent years, search-based approaches have begun to explore improving software to consume less energy. Machine learning is a particularly strong candidate for this because it is possible to trade off functionality (accuracy) against energy consumption, whereas with many programs functionality is simply a pass-or-fail constraint. We use a grid search to explore hyperparameter configurations for a multilayer perceptron on five classification data sets, considering trade-offs of classification accuracy against training or inference energy. On one data set, we show that 77 percent of energy consumption for inference can be saved by reducing accuracy from 94.3 to 93.2 percent. Energy for training can also be reduced by 30-50 with minimal loss of accuracy. We also find that structural parameters like hidden layer size is a major driver of the energy-accuracy trade-off, but there is some evidence that non-structural hyperparameters influence the trade-off too. We also show that a search-based approach has the potential to identify these tradeoffs more efficiently than the grid search.",
    "author": "Alexander E. I. Brownlee, Jason Adair, Saemundur O. Haraldsson, John Jabbo",
    "doi": "10.1109/GI52543.2021.00011",
    "journal": "ACM International Workshop on Genetic Improvement (GI)",
    "keywords": "Training, Energy consumption, Greenhouse effect, Neurons, Multilayer perceptrons ,Software, Hardware",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "Exploring the Accuracy–Energy Trade-off in Machine Learning",
    "type": "article",
    "url": "https://doi.org/10.1109/GI52543.2021.00011",
    "volume": "",
    "year": "2021"
  },
  "DarvishRouhani2016DeLight": {
    "abstract": "Physical viability, in particular energy efficiency, is a key challenge in realizing the true potential of Deep Neural Networks (DNNs). In this paper, we aim to incorporate the energy dimension as a design parameter in the higher-level hierarchy of DNN training and execution to optimize for the energy resources and constraints. We use energy characterization to bound the network size in accordance to the pertinent physical resources. An automated customization methodology is proposed to adaptively conform the DNN configurations to the underlying hardware characteristics while minimally affecting the inference accuracy. The key to our approach is a new context and resource aware projection of data to a lower-dimensional embedding by which learning the correlation between data samples requires significantly smaller number of neurons. We leverage the performance gain achieved as a result of the data projection to enable the training of different DNN architectures which can be aggregated together to further boost the inference accuracy. Accompanying APIs are provided to facilitate rapid prototyping of an arbitrary DNN application customized to the underlying platform. Proof-of-concept evaluations for deployment of different visual, audio, and smart-sensing benchmarks demonstrate up to 100-fold energy improvement compared to the prior-art DL solutions.",
    "author": "Behnam Darvish Rouhani, Amir Mirhoseini, Farinaz Koushanfar",
    "doi": "10.1145/2934583.2934599",
    "journal": "International Symposium on Low Power Electronics and Design",
    "keywords": "deep neural networks, energy efficiency, energy-aware training, data projection, dimensionality reduction",
    "number": "",
    "publisher": "ACM",
    "series": "",
    "title": "DeLight: Adding Energy Dimension to Deep Neural Networks",
    "type": "article",
    "url": "https://doi.org/10.1145/2934583.2934599",
    "volume": "",
    "year": "2016"
  },
  "Lazzaro2023EnergyAwareTraining": {
    "abstract": "Deep learning models undergo a significant increase in the number of parameters they possess, leading to the execution of a larger number of operations during inference. This expansion significantly contributes to higher energy consumption and prediction latency. In this work, we propose EAT, a gradient-based algorithm that aims to reduce energy consumption during model training. To this end, we leverage a differentiable approximation of the norm, and use it as a sparse penalty over the training loss. Through our experimental analysis conducted on three datasets and two deep neural networks, we demonstrate that our energy-aware training algorithm EAT is able to train networks with a better trade-off between classification performance and energy efficiency.",
    "author": "Dario Lazzaro, Antonio Emanuele Cinà, Maura Pintor, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo",
    "doi": "10.1007/978-3-031-43153-1_43",
    "journal": "Image Analysis and Processing – ICIAP 2023",
    "keywords": "Training, hardware acceleration, energy efficiency, sparsity, maximization, regularization",
    "number": "",
    "publisher": "Springer",
    "series": "",
    "title": "Minimizing Energy Consumption of Deep Learning Models by Energy-Aware Training",
    "type": "article",
    "url": " https://doi.org/10.1007/978-3-031-43153-1_43",
    "volume": "",
    "year": "2023"
  },
  "Hodak2021RecentEfficiency": {
    "abstract": "Deep learning (DL) continues to develop at a rapid pace with improvements coming from both hardware and software sides. In this work we evaluate the strides made over the last 2 years, during which a new generation of GPU accelerators has been introduced and significant algorithmic progress has been made. We find a dramatic improvement in runtime and power usage for a standard AI training workload. Specifically, a 4x improvement in runtime and energy consumption is demonstrated. The improvements are about equally split between hardware and algorithms. Additionally, we examine further ways to improve AI training power consumption on data center servers and identity 3 system level tunings that make most difference. These yield up to 20 percent more energy savings without any changes to the user code. Implications for the field and ways to make DL more energy-efficient going forward are also discussed",
    "author": "Miro Hodak, Ajay Dholakia",
    "doi": "10.1109/BigData52589.2021.9671762",
    "journal": "IEEE International Conference on Big Data",
    "keywords": "Training, Deep learning, Runtime, Software algorithms, Big Data, Hardware, Artificial intelligence",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "Recent Efficiency Gains in Deep Learning: Performance, Power, and Sustainability",
    "type": "article",
    "url": "https://doi.org/10.1109/BigData52589.2021.9671762",
    "volume": "",
    "year": "2021"
  },
  "Tu2023UnveilingEnergyEfficiency": {
    "abstract": "Today, deep learning optimization is primarily driven by research focused on achieving high inference accuracy and reducing latency. However, the energy efficiency aspect is often overlooked, possibly due to a lack of sustainability mindset in the field and the absence of a holistic energy dataset. In this paper, we conduct a threefold study, including energy measurement, prediction, and efficiency scoring, with an objective to foster transparency in power and energy consumption within deep learning across various edge devices. Firstly, we present a detailed, first-of-its-kind measurement study that uncovers the energy consumption characteristics of on-device deep learning. This study results in the creation of three extensive energy datasets for edge devices, covering a wide range of kernels, state-of-the-art DNN models, and popular AI applications. Secondly, we design and implement the first kernel-level energy predictors for edge devices based on our kernel-level energy dataset. Evaluation results demonstrate the ability of our predictors to provide consistent and accurate energy estimations on unseen DNN models. Lastly, we introduce two scoring metrics, PCS and IECS, developed to convert complex power and energy consumption data of an edge device into an easily understandable manner for edge device end-users. We hope our work can help shift the mindset of both end-users and the research community towards sustainability in edge computing, a principle that drives our research.",
    "author": "Xiaolong Tu, Anik Mallik, Dawei Chen, Kyungtae Han, Onur Altintas, Haoxin Wang, Jiang Xie",
    "doi": "10.1145/3583740.3628442",
    "journal": "IEEE Symposium on Edge Computing",
    "keywords": "Edge AI, Deep Neural Network, Energy Consumption",
    "number": "",
    "publisher": "ACM",
    "series": "",
    "title": "Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring Across Edge Devices",
    "type": "article",
    "url": "https://doi.org/10.1145/3583740.3628442",
    "volume": "",
    "year": "2023"
  },
  "Yang2017EnergyEfficientCNN": {
    "abstract": "Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or the amount of computation, we find that they do not necessarily result in lower energy consumption. Therefore, these targets do not serve as a good metric for energy cost estimation. To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses the energy consumption of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in the output feature maps instead of the filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet is reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. We also show that reducing the number of target classes in AlexNet greatly decreases the number of weights, but has a limited impact on energy consumption.",
    "author": "Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze",
    "doi": "10.1109/CVPR.2017.643",
    "journal": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
    "keywords": "Energy consumption , Memory management , Hardware , Estimation , Computational modeling , Measurement , Smart phones",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning",
    "type": "article",
    "url": "https://doi.org/10.1109/CVPR.2017.643",
    "volume": "",
    "year": "2017"
  },
  "Yarally2023GreenAI": {
    "abstract": "Modern AI practices all strive towards the same goal: better results. In the context of deep learning, the term results often refers to the achieved accuracy on a competitive problem set. In this paper, we adopt an idea from the emerging field of Green AI to consider energy consumption as a metric of equal importance to accuracy and to reduce any irrelevant tasks or energy usage. We examine the training stage of the deep learning pipeline from a sustainability perspective, through the study of hyperparameter tuning strategies and the model complexity, two factors vastly impacting the overall pipeline’s energy consumption. First, we investigate the effectiveness of grid search, random search and Bayesian optimisation during hyperparameter tuning, and we find that Bayesian optimisation significantly dominates the other strategies. Furthermore, we analyse the architecture of convolutional neural networks with the energy consumption of three prominent layer types: convolutional, linear and ReLU layers. The results show that convolutional layers are the most computationally expensive by a strong margin. Additionally, we observe diminishing returns in accuracy for more energy-hungry models. The overall energy consumption of training can be halved by reducing the network complexity. In conclusion, we highlight innovative and promising energy-efficient practices for training deep learning models. To expand the application of Green AI, we advocate for a shift in the design of deep learning models, by considering the trade-off between energy efficiency and accuracy.",
    "author": "Tim Yarally, Luís Cruz, Daniel Feitosa, June Sallou, Arie van Deursen",
    "doi": "10.1109/CAIN58948.2023.00012",
    "journal": "2023 IEEE/ACM 2nd International Conference on AI Engineering – Software Engineering for AI (CAIN)",
    "keywords": "Deep learning , Training , Energy consumption , Computational modeling , Computer architecture , Energy efficiency , Bayes methods",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "Uncovering Energy-Efficient Practices in Deep Learning Training: Preliminary Steps Towards Green AI",
    "type": "article",
    "url": "https://doi.org/10.1109/CAIN58948.2023.00012",
    "volume": "",
    "year": "2023"
  },
  "Tschand2025MLPerfPower": {
    "abstract": "Rapid adoption of machine learning (ML) technologies has led to a surge in power consumption across diverse systems, from tiny IoT devices to massive datacenter clusters. Benchmarking the energy efficiency of these systems is crucial for optimization, but presents novel challenges due to the variety of hardware platforms, workload characteristics, and system-level interactions. This paper introduces MLPerf® Power, a comprehensive benchmarking methodology with capabilities to evaluate the energy efficiency of ML systems at power levels ranging from microwatts to megawatts. Developed by a consortium of industry professionals from more than 20 organizations, coupled with insights from academia, MLPerf Power establishes rules and best practices to ensure comparability across diverse architectures. We use representative workloads from the MLPerf benchmark suite to collect 1,841 reproducible measurements from 60 systems across the entire range of ML deployment scales. Our analysis reveals trade-offs between performance, complexity, and energy efficiency across this wide range of systems, providing actionable insights for designing optimized ML solutions from the smallest edge devices to the largest cloud infrastructures. This work emphasizes the importance of energy efficiency as a key metric in the evaluation and comparison of the ML system, laying the foundation for future research in this critical area. We discuss the implications for developing sustainable AI solutions and standardizing energy efficiency benchmarking for ML systems.",
    "author": "Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly",
    "doi": "10.1109/HPCA61900.2025.00092",
    "journal": "2025 IEEE International Symposium on High Performance Computer Architecture (HPCA)",
    "keywords": "Performance evaluation,Power demand,Power measurement,Standards organizations,Machine learning,Benchmark testing,Energy efficiency,Surges,Optimizatio,System analysis and design",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from µWatts to MWatts for Sustainable AI",
    "type": "article",
    "url": "https://doi.org/10.1109/HPCA61900.2025.00092",
    "volume": "",
    "year": "2025"
  },
  "Rajput2024BenchmarkingQuantization": {
    "abstract": "In the era of generative artificial intelligence (AI), the quest for energy-efficient AI models is increasing. The increasing size of recent AI models has led to quantization techniques that reduce large models' computing and memory requirements. This study aims to compare the energy consumption of five quantization methods, viz. Gradient-based Post-Training Quantization (GPTQ),Activation-aware Weight Quantization (AWQ), GPT-Generated Model Language (GGML), GPT-Generated Unified Format (GGUF), and Bits and Bytes (BNB). We benchmark and analyze the energy efficiency of these commonly used quantization methods during inference. This preliminary exploration found that GGML and its successor GGUF were the most energy-efficient quantization methods. Our findings reveal significant variability in energy profiles across methods, challenging the notion that lower precision universally improves efficiency. The results underscore the need to benchmark quantization techniques from an energy perspective beyond just model compression. Our findings could guide the selection of models using quantization techniques and the development of new quantization techniques that prioritize energy efficiency, potentially leading to more environmentally friendly AI deployments.",
    "author": "P. Rajput, M. Sharma",
    "doi": "10.1109/ICSA-C63560.2024.00049",
    "journal": "2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C)",
    "keywords": "Deep learning, Energy consumption, Quantization (signal), Software architecture, Generative AI, Computational modeling, Memory management",
    "number": "",
    "publisher": "IEEE",
    "series": "",
    "title": "Benchmarking Emerging Deep Learning Quantization Methods for Energy Efficiency",
    "type": "article",
    "url": "https://doi.org/10.1109/ICSA-C63560.2024.00049",
    "volume": "",
    "year": "2024"
  },
    "Menghani2023EfficientDeepLearning": {
    "abstract": "Deep learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval, and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, and resources required to train, among others, have all increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. It is our hope that this survey would provide readers with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.",
    "author": "Gaurav Menghani",
    "doi": "10.1145/3578938",
    "journal": "ACM Computing Surveys",
    "keywords": "Efficient deep learning, efficient machine learning, efficient artificial intelligence, quantization, pruning, sparsity, distillation, model compression, model optimization",
    "number": "12",
    "publisher": "ACM",
    "series": "",
    "title": "Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better",
    "type": "article",
    "url": "https://doi.org/10.1145/3578938",
    "volume": "55",
    "year": "2023"
  }
};